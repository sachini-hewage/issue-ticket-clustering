{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7cdddfc-7b79-430d-9ef8-508607b3b830",
   "metadata": {},
   "source": [
    "## <span style=\"color:#0066cc; font-weight:bold\">INFERENCE PIPELINE FOR NEW TICKET CLASSIFICATION AND NEXT STEP RECOMMENDATION</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c540a202-4075-4ea7-841b-4f7e4f383cf3",
   "metadata": {},
   "source": [
    "<p style=\"font-size:18px;\">\n",
    "This notebook orchestrates the total pipeline for a new ticket that is saved on PostgreSQL tickets table, using its ticket_id.<br> \n",
    "I am using some of the 128 tickets that I did not touch for any preprocessing, clustering or recommendations that I have marked with <strong>demo_flag = TRUE</strong>.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fba19cb-49eb-401a-9ad4-e1b59318c649",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4da6ff; font-weight:bold\">Demo setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb58ca0-3d11-448b-baab-a1388e59bad8",
   "metadata": {},
   "source": [
    "<p style=\"font-size:16px;\">This setup involes cherry-picking at least 3 tickets( each represnting EN, SV, FI) from the \"ticket\" table where demo_flag = TRUE.\n",
    "These tickets have not been preprocessed, embedded or previously seen by any models downstream.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c78d79-97b5-403a-9aa6-4157d7fd0983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add project root to path \n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(str(Path(\"../\").resolve()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad07fbc-5960-4d48-a449-6430460b2b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generic imports\n",
    "import pandas as pd \n",
    "\n",
    "\n",
    "# imports from my project\n",
    "import src.preprocessing_utils as prep\n",
    "from src.db_utils import fetch_and_cleanup_tickets\n",
    "import src.preprocessing_utils as prep\n",
    "from src.db_utils import fetch_preprocessed_tickets, fetch_already_embedded_ticket_ids, insert_embeddings\n",
    "from src.embedding_utils import batch_embed_and_store,embed_new_tickets\n",
    "import src.recommendation_utils as ru\n",
    "import src.diagnostic_utils as dgn\n",
    "\n",
    "\n",
    "# Show full column content\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069659fc-3782-4dfb-ad4a-82083a401aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify ticket_id for demo tickets\n",
    "\n",
    "# English example : \"TKT-524587\"\n",
    "# Finish example: \"TKT-537091\"\n",
    "# Swedish example: \"TKT-520235\"\n",
    "# Absurd ticket: \"TKT-600000\"\n",
    "\n",
    "ticket_ids = [\"TKT-524587\", \"TKT-537091\", \"TKT-520235\", \"TKT-600000\" ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759327e0-f2ea-4053-b3a1-266c7a23b507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's fetch the relevant records from the table \"tickets\".\n",
    "\n",
    "new_tickets = fetch_and_cleanup_tickets(ticket_ids)\n",
    "new_tickets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542894d8-e083-4b6f-a6b7-824a20aac743",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4da6ff; font-weight:bold\">Preprocessing tickets</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1775c6-4695-4d5c-9427-1209537a1417",
   "metadata": {},
   "source": [
    "<p style=\"font-size:16px;\">\n",
    "In this section we will preprocess and write the results to the table <code>ticket_preprocessed</code>. There are 7 steps altogether:\n",
    "</p>\n",
    "<ol style=\"font-size:16px;\">\n",
    "    <li>Language detection with langdetect</li>\n",
    "    <li>Text cleaning (removal of emojis and extra spacing)</li>\n",
    "    <li>Translation to English using <code>Helsinki-NLP/opus-mt-sv-en</code> / <code>Helsinki-NLP/opus-mt-fi-en</code></li>\n",
    "    <li>PII masking for names, locations, card numbers, telephone numbers, IBAN and email</li>\n",
    "    <li>Combine cleaned and translated subject and body</li>\n",
    "    <li>Keyword extraction from the above combination for verification purposes</li>\n",
    "    <li>Write the preprocessed ticket data to PostgreSQL</li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adaec3fd-966f-4e56-877d-72ed678a7a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_tickets = prep.preprocess_tickets(new_tickets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09e5f4f-c59a-43eb-905a-c34be8f8eec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_tickets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0913666d-cf22-45fd-bd35-c2a5be91784e",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4da6ff; font-weight:bold\">Embedding combined_text (subject + body)</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ed08c4-51c9-4775-8f01-870c8486bbc3",
   "metadata": {},
   "source": [
    "<ul style=\"font-size:16px;\">\n",
    "    <li>Now I am going to embed the <code>combined_text</code> field from <code>tickets_preprocessed</code> table.</li>\n",
    "    <li>This field is <code>{translated_subject || masked_body}</code> which I feel is sufficient to categorize issue types.</li>\n",
    "    <li>The embeddings I use is <code>all-MiniLM-L6-v2</code> from SentenceTransformers which outputs a 384-dimensional vector.</li>\n",
    "    <li>This vector will also be saved on the <code>ticket_embedding</code> table on PostgreSQL for later steps.</li>\n",
    "</ul>\n",
    "\n",
    "<p style=\"font-size:16px;\"><strong>Note:</strong> The embedding model is configurable (change <code>src.configs.LOCAL_EMBEDDING_MODEL</code> and the vector size of the <code>embedding</code> column).</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0084483d-897c-4617-8c38-57f7c24e8b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_new_tickets()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9acc2f-29d2-4543-afd1-1709aa9d712c",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4da6ff; font-weight:bold\">Using saved HDBSCAN model to predict a new ticket's category and next step recommendation </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4df7faf-1bb7-42f3-bef4-0d84add11e5f",
   "metadata": {},
   "source": [
    "<p style=\"font-size:16px;\">\n",
    "The output dimension of the embeddings (384) is too high for clustering, since distance measures become less meaningful and clustering itself scales poorly. \n",
    "I have used <strong>UMAP (Uniform Manifold Approximation and Projection)</strong> to project the embeddings into a reasonable dimension before clustering. \n",
    "This projector has been saved as a <code>.pkl</code> file so that any new tickets go through the exact same projector before being categorized by the clustering model.\n",
    "</p>\n",
    "\n",
    "<p style=\"font-size:16px;\">\n",
    "I have implemented a clusterer using <strong>HDBSCAN (Hierarchical Density-Based Spatial Clustering of Applications with Noise)</strong> because:\n",
    "</p>\n",
    "<ol style=\"font-size:16px;\">\n",
    "    <li>The number of clusters, their shapes, and densities are unknown.</li>\n",
    "    <li>The dataset could be noisy and contain many outliers.</li>\n",
    "</ol>\n",
    "\n",
    "<p style=\"font-size:16px;\">\n",
    "The generated and saved clusterer should be loaded for this section. It will be used to categorize new tickets using <code>approximate_predict</code> in HDBSCAN. \n",
    "Note that even a new ticket with a previously unseen issue category will still fall into an existing cluster but with a <strong><span style=\"color:red;\">very low confidence score. </span></strong>\n",
    "</p>\n",
    "\n",
    "<p style=\"font-size:16px;\">\n",
    "I maintain a <code>cluster_labels</code> table which has human-readable issue category names. \n",
    "A local LLM (<code>mistral:7b</code> via Ollama currently) is prompted to inspect the <code>combined_text</code> of the top-N (10 currently) tickets closest to each cluster center and issue a suitable natural language label. \n",
    "</p>\n",
    "\n",
    "<p style=\"font-size:16px;\">\n",
    "After the issue categorization, the recommender program will:\n",
    "</p>\n",
    "<ol style=\"font-size:16px;\">\n",
    "    <li>Find the closest ticket in the same cluster.</li>\n",
    "    <li>Prefer resolved/closed tickets; otherwise use the nearest open ticket.</li>\n",
    "    <li>Compare <code>internal_comments</code> via an LLM (<code>mistral:7b</code> via Ollama currently) to suggest the next recommended step. \n",
    "    This could be knowledge from the neighbor or general advice depending on the neighbor_confidence (I am using > 0.8).</li>\n",
    "</ol>\n",
    "\n",
    "<p style=\"font-size:16px;\"> <strong>Note:</strong> The local model can be configured via <code>src.ai_utils</code>.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b88c56-06e3-407f-ac41-cb6bedbfa0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store results\n",
    "results = []\n",
    "\n",
    "# Get cluster id, label, and suggested next step for each ticket\n",
    "for tid in ticket_ids:\n",
    "    (\n",
    "        suggestion,\n",
    "        c_confidence,\n",
    "        n_confidence,\n",
    "        cluster_id,\n",
    "        cluster_label,\n",
    "        best_comments,\n",
    "        nearest_ticket_id,\n",
    "        nearest_ticket_status,\n",
    "        internal_comments\n",
    "    ) = ru.find_recommendation(tid)\n",
    "\n",
    "\n",
    "    results.append({\n",
    "        \"ticket_id\": tid,\n",
    "        \"Cluster ID\": cluster_id,\n",
    "        \"Cluster Label\": cluster_label,\n",
    "        \"Cluster confidence\": round(c_confidence, 2),\n",
    "        \"Neighbour confidence\": round(n_confidence, 2),\n",
    "        \"Nearest Ticket\": nearest_ticket_id,\n",
    "        \"Nearest Ticket Status\": nearest_ticket_status,\n",
    "        \"Neighbouring Action Sequence\": best_comments,\n",
    "        \"Suggested Next Step\": suggestion,\n",
    "        \"Internal Comments\": internal_comments\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "# Join with preprocessed_tickets to add combined_text \n",
    "if 'ticket_id' not in preprocessed_tickets.columns:\n",
    "    raise KeyError(\"`preprocessed_tickets` must contain a 'ticket_id' column for joining.\")\n",
    "\n",
    "# Select only needed columns to avoid duplication or bloat\n",
    "df_joined = df_results.merge(\n",
    "    preprocessed_tickets[[\"ticket_id\", \"combined_text\"]],\n",
    "    on=\"ticket_id\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Define desired column order\n",
    "column_order = [\n",
    "    \"ticket_id\",\n",
    "    \"Cluster ID\",\n",
    "    \"Cluster Label\",\n",
    "    \"combined_text\",\n",
    "    \"Cluster confidence\",\n",
    "    \"Nearest Ticket\",\n",
    "    \"Nearest Ticket Status\",\n",
    "    \"Neighbouring Action Sequence\",\n",
    "    \"Internal Comments\",\n",
    "    \"Suggested Next Step\",\n",
    "    \"Neighbour confidence\"\n",
    "]\n",
    "\n",
    "# Reorder columns\n",
    "df_joined = df_joined[column_order]\n",
    "\n",
    "# Display final DataFrame\n",
    "display(df_joined)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa75124-2652-404e-a89c-b8538fc73e95",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4da6ff; font-weight:bold\">Results validation</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b955a4-1be1-4e58-a548-e099025648dc",
   "metadata": {},
   "source": [
    "<div style=\"font-size:16px;\">\n",
    "    <p>\n",
    "        This section demonstrates some of the possible diagnostic tools to make sense of why results are the way they are.\n",
    "    </p>\n",
    "    <ul>\n",
    "        <li><strong>Diagnostic step 1:</strong> Fetches the <code>subject_translated</code> and <code>body_translated</code> for a given ticket and its closest neighbour for manual comparison and verification by a domain expert.</li>\n",
    "        <li><strong>Diagnostic step 2:</strong> Reduces the embedding dimension to 2D and visualises all clusters, highlighting a given ticket. </li>\n",
    "        <li><strong>Diagnostic step 3:</strong> Reduces the embedding dimension to 2D and visualises a given cluster, highlighting a given ticket and its neighbour.</li>\n",
    "    </ul>\n",
    "</div>\n",
    "<p style=\"font-size:16px;\"> <strong>Note:</strong> You can add more diagnostic functions as required to <code>src.diagnostic_utils</code>.</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab215ab-61d2-47d2-b7ce-871e9731e203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide the focus for diagnostics from the above results here. \n",
    "ticket_id = \"TKT-520235\"\n",
    "neighbour_id = \"TKT-543369\"\n",
    "cluster_id = 22"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e1ca49-5318-4c7d-9bc1-a9ca0ac66a71",
   "metadata": {},
   "source": [
    "<div style=\"font-size:16px;\"><strong>Diagnostic step 1:</strong></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26163f1-136f-497c-b83c-401390e769eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch subject_translated and body_translated for a given ticket and its closest neighbour\n",
    "dgn_df = dgn.fetch_ticket_pair_details(ticket_id, neighbour_id, cluster_id)\n",
    "dgn_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f41919-f0b4-4513-87aa-f83c451746a4",
   "metadata": {},
   "source": [
    "<div style=\"font-size:16px;\"><strong>Diagnostic step 2:</strong></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6c1f63-f4c9-4dab-beda-484887c611ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "dgn.plot_all_clusters(ticket_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224262c4-ccd6-4ad4-86e9-7086dcfeb291",
   "metadata": {},
   "source": [
    "<div style=\"font-size:16px;\"><strong>Diagnostic step 3:</strong></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f28ef27-a0e2-4cf2-8a3d-d656e121695a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dgn.plot_cluster_projection(ticket_id, neighbour_id, cluster_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc8a6ce-6363-476c-b1f0-de65ebb65ab7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61252967-ca33-49b2-b911-6781be1275a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd21ea7e-cf52-4f15-bf95-50f77f9e63c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (enfuce-ai)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
